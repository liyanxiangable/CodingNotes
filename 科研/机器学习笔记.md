# 机器学习入门教程笔记 1

本文基于 tensorflow 官方教程，仅作为个人学习记录。对机器学习感兴趣的同学建议阅读其他有价值文章。

## 环境搭建

首先 python 环境已经搭建好了。下面在 win10 环境下安装 tensorflow：

```shell
pip install tensorflow
```

以上命令是安装的 CPU 版本的 tensorflow，另外还有一个基于 CUDA 的 GPU 版本，但是由于我的显卡算力不足并且仅支持 linux 系统（实际上我的 CPU 也是渣），于是采用 CPU 1.8 版本。

注意如果报错：PermissionError: [WinError 5] 拒绝访问。: 'c:\\programdata\\anaconda3\\lib\\site-packages\\html5lib-1.0.1-py3.6.egg-info\\dependency_links.txt'。那么应当使用管理员身份启动 cmd 再进行安装。

以上安装步骤完成之后，可以在 python 中进行如下简单测试：

```python
import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
```

如果没有报错并成功输出信息，那么 tensorflow 就安装好了。但是在 pycharm 环境中运行会有一个提示：Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

查了一下相关的问题处理，说是关闭警告。。。。可以使用如下代码：

```python
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
```

这样就有些掩耳盗铃了，我建议还是使用源码编译：

首先卸载已经安装的 tensorflow：

```shell
pip uninstall tensorflow
```

然后在 tensorflow 的 [github](https://github.com/tensorflow/tensorflow) 上下载对应 python 版本的 whl 源码文件。

注意我一开始下载的是  [Python 3.6 64-bit](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tf_nightly-1.head-cp36-cp36m-win_amd64.whl)，安装不上，于是转为下载 build history 中的 [tf_nightly-1.8.0.dev20180409-cp36-cp36m-win_amd64.whl](https://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows,PY=36/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tf_nightly-1.8.0.dev20180409-cp36-cp36m-win_amd64.whl)。

安装过程中出现依赖缺失：

notebook 5.4.0 requires ipykernel, which is not installed.
jupyter 1.0.0 requires ipykernel, which is not installed.
jupyter-console 5.2.0 requires ipykernel, which is not installed.
ipywidgets 7.1.1 requires ipykernel>=4.5.1, which is not installed.

于是安装 ipykernel。之后再运行上边的小栗子，还是会出现那个警告。放弃。。先凑合着使吧。



## TensorFlow 使用基础

### 概述

• 使用图 (graphs) 来表示计算.
• 在会话 (Session) 中执行图.
• 使用张量 (tensors) 来代表数据.
• 通过变量 (Variables) 维护状态.
• 使用供给 (feeds) 和取回 (fetches) 将数据传入或传出任何操作.

TensorFlow是一个以图(graphs)来表示计算的编程系统,图中的节点被称之为op (operation的缩写). 一个 op 获得零或多个张量 (tensors) 执行计算, 产生零或多个张量。张量是一个按类型划分的多维数组。例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是[batch, height, width, channels]。

TensorFlow 的图是一种对计算的抽象描述。在计算开始前, 图必须在 会话 (Session()) 中被启动. 会话将图的 op 分发到如 CPU 或 GPU 之类的 设备 (Devices()) 上, 同时提供执行 op 的方法。这些方法执行后, 将产生的张量 (tensor) 返回。在 Python 语言中, 将返回numpy的ndarray 对象; 在 C 和 C++ 语言中, 将返回tensorflow::Tensor实例。

### The computation graph | 计算图

通常，TensorFlow 编程可按两个阶段组织起来: 构建阶段和执行阶段; 前者用于组
织计算图，而后者利用 session 中执行计算图中的 op 操作。例如, 在构建阶段创建一个图来表示和训练神经网络，然后在执行阶段反复执行一组 op 来实现图中的训练。TensorFlow 支持 C、C++、Python 编程语言。目前, TensorFlow 的 Python 库更加易用, 它提供了大量的辅助函数来简化构建图的工作, 而这些函数在 C 和 C++ 库中尚不被支持。这三种语言的会话库 (session libraries) 是一致的.

*TensorFlow 编程分为两个阶段是考虑效率，即构建阶段是对整个算法与步骤的描述，然后执行阶段是集中对算法步骤进行执、进行运算*

### Building the graph | 构建计算图

刚开始基于 op 建立图的时候一般不需要任何的输入源 (source op)，例如输入常量(Constance)，再将它们传递给其它 op 执行运算。
Python 库中的 op 构造函数返回代表已被组织好的 op 作为输出对象，这些对象可以传递给其它 op 构造函数作为输入。
TensorFlow Python 库有一个可被 op 构造函数加入计算结点的默认图(default graph)。对大多数应用来说，这个默认图已经足够用了。阅读 Graph 类文档来了解如何明晰的管理多个图.

```python
import tensorflow as tf

# 创建一个 1x2 的常量矩阵. 这个结点会自动加入计算图
matrix1 = tf.constant([[3., 3.]])
# Create another Constant that produces a 2x1 matrix.
matrix2 = tf.constant([[2.],[2.]])

# 创建一个输入为 'matrix1' 与 'matrix2' 的矩阵相乘结点（op）
product = tf.matmul(matrix1, matrix2)
```

API:

```python
tf.constant(value, dtype=None, shape=None, name='Const')
创建一个常量结点。
返回的张量是一个由 dtype 类型组成的，具有明确的参数并可选设定尺寸形状的常量结点

参数可以是常量，或者是一个 dtype 类型值的列表。如果值是一个列表，那么这个列表的长度必须小于等于尺寸形状参数所限制的元素个数。列表的长度小于给定的形状的元素个数的情况下，列表当中的最后一个元素就会用来填充剩余的位置。

尺寸形状的参数是可选的。如果有这个参数，那么他就表示返回的张量的维数；如果没有这个参数，那么返回的张量视 value 参数情况为一个标量或者是一个一维的矢量。

如果 dtype 参数没有指定，那么类型参数将会由 value 参数而推断出

示例：

 # Constant 1-D Tensor populated with value list.
 tensor = tf.constant([1, 2, 3, 4, 5, 6, 7]) => [1 2 3 4 5 6 7]

 # Constant 2-D tensor populated with scalar value -1.
 tensor = tf.constant(-1.0, shape=[2, 3]) => [[-1. -1. -1.]
                                              [-1. -1. -1.]]
Args:
value: A constant value (or list) of output type dtype.
dtype: The type of the elements of the resulting tensor.
shape: Optional dimensions of resulting tensor.
name: Optional name for the tensor.
Returns:
A Constant Tensor.
```

```
tf.matmul(a, b, transpose_a=False, transpose_b=False, a_is_sparse=False, b_is_sparse=False, name=None)
Multiplies matrix a by matrix b, producing a * b.

输入必须一个或者多个为二维的矩阵，并且矩阵的尺寸（经过转置之后）要能够正确相乘，

矩阵必须要有相同的类型，支持的类型包括: float, double, int32, complex64.

矩阵可以通过设定标志位为 true 进行临时的转置，当然默认情况下是 false 的。

如果一个或者两个矩阵都还有许多的 0 元素，那么可以通过设定 a_is_sparse 等标志位来进行高效率的矩阵相乘算法，默认是 false 的。

示例:

# 2-D tensor `a`
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]
                                                      [4. 5. 6.]]
# 2-D tensor `b`
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]
                                                         [9. 10.]
                                                        [11.12.]]
c = tf.matmul(a, b) => [[58 64]
                        [139 154]]
Args:
a: Tensor of type float, double, int32 or complex64.
b: Tensor with same type as a.
transpose_a: If True, a is transposed before multiplication.
transpose_b: If True, b is transposed before multiplication.
a_is_sparse: If True, a is treated as a sparse matrix.
b_is_sparse: If True, b is treated as a sparse matrix.
name: Name for the operation (optional).
Returns:
A Tensor of the same type as a.
```

默认图现在拥有三个节点，两个constant() op，一个matmul() op. 为了真正进行矩阵乘法运算，得到乘法结果, 你必须在一个会话 (session) 中载入这个图。

### Launching the graph in a session | 在会话中载入图

构建过程完成后就可运行执行过程。为了载入之前所构建的图，必须先创建一个
会话对象 (Session object)。会话构建器在未指明参数时会载入默认的图。
完整的会话 API 资料，请参见会话类 (Session object)。

```python
# 启动默认计算图.

sess = tf.Session()

# 为了运行图中的矩阵相乘结点 matmul op，我们需要调用会话的 run 方法，并传递一个 product 变量，这个 product 变量代表了 matmul op 结点的输出结果。传入它是向方法表明, 我们希望取回矩阵乘法 op 的输出.

# 所有的 op 结点中需要的输入都是在会话中自动执行的，它们会并行计算。

# 返回值 'result' 是一个 numpy `ndarray` 对象.

result = sess.run(product)

print(result)

# ==> [[ 12.]]

# 完成之后关闭会话

sess.close()
```

会话在完成后必须关闭以释放资源。你也可以使用"with"句块开始一个会话，该会
话将在"with"句块结束时自动关闭。

```python
with tf.Session() as sess:
result = sess.run([product])
print(result)
```

TensorFlow 事实上通过一个“翻译”过程，将定义的图转化为不同的可用计算资源
间实现分布计算的操作，如 CPU 或是显卡 GPU。通常不需要用户指定具体使用的 CPU或 GPU，TensorFlow 能自动检测并尽可能的充分利用找到的第一个 GPU 进行运算。

如果你的设备上有不止一个 GPU，你需要明确指定 op 操作到不同的运算设备以调用它们。使用with...Device语句明确指定哪个 CPU 或 GPU 将被调用:

```python
with tf.Session() as sess:
with tf.device("/gpu:1"):
matrix1 = tf.constant([[3., 3.]])
matrix2 = tf.constant([[2.],[2.]])
product = tf.matmul(matrix1, matrix2)
...
```

### Interactive Usage | 交互式使用

文档中的 Python 示例使用一个会话 Session 来启动图, 并调用 Session.run() 方法执行操作。
考虑到如IPython这样的交互式 Python 环境的易用, 可以使用InteractiveSession 代替Session类, 使用 Tensor.eval()和 Operation.run() 方法代替 Session.run(). 这样可以避免使用一个变量来持有会话.

```python
# Enter an interactive TensorFlow Session.
import tensorflow as tf
sess = tf.InteractiveSession()
x = tf.Variable([1.0, 2.0])
a = tf.constant([3.0, 3.0])

# Initialize 'x' using the run() method of its initializer op.
x.initializer.run()

# Add an op to subtract 'a' from 'x'. Run it and print the result
sub = tf.sub(x, a)
print(sub.eval())
# ==> [−2. −1.]

# Close the Session when we're done.
sess.close()
```

###  Tensors | 张量

TensorFlow 程序使用 tensor 数据结构来代表所有的数据, 计算图中, 操作间传递的数据都是 tensor. 你可以把 TensorFlow 的张量看作是一个 n 维的数组或列表. 一个 tensor 包含一个静态类型 rank, 和一个 shape. 想了解 TensorFlow 是如何处理这些概念的, 参见 Rank, Shape, 和 Type]。

### Variables | 变量

变量维持了图执行过程中的状态信息。下面的例子演示了如何使用变量实现一个
简单的计数器，更多细节详见变量章节。

```python
# 建立一个变量， 用 0 初始化它的值
state = tf.Variable(0, name="counter")

# 创建一个常量
one = tf.constant(1)

new_value = tf.add(state, one)
update = tf.assign(state, new_value)

# Variables must be initialized by running an `init` Op after having launched the graph. We first have to add the `init` Op to the graph.
init_op = tf.initialize_all_variables()

# Launch the graph and run the ops.
with tf.Session() as sess:
# Run the 'init' op
sess.run(init_op)
# Print the initial value of 'state'
print(sess.run(state))
# Run the op that updates 'state' and print 'state'.
for _ in range(3):
	sess.run(update)
	print(sess.run(state))

# output:
# 0
# 1
# 2
# 3
```

代码中assign()操作是图所描绘的表达式的一部分, 正如add()操作一样. 所以在调
用run()执行表达式之前, 它并不会真正执行赋值操作.通常会将一个统计模型中的参数表示为一组变量. 例如, 你可以将一个神经网络的权重作为某个变量存储在一个 tensor 中. 在训练过程中, 通过重复运行训练图, 更新这个 tensor.

### Fetches | 取回

为了取回操作的输出内容, 可以在使用 Session 对象的 run() 调用执行图时, 传入一些 tensor, 这些 tensor 会帮助你取回结果. 在之前的例子里, 我们只取回了单个节点state, 但是你也可以取回多个 tensor:

```python
input1 = tf.constant(3.0)
input2 = tf.constant(2.0)
input3 = tf.constant(5.0)
intermed = tf.add(input2, input3)
mul = tf.mul(input1, intermed)
with tf.Session() as sess:
result = sess.run([mul, intermed])
print(result)

# output:
# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]
```

需要获取的多个 tensor 值，在 op 的一次运行中一起获得（而不是逐个去获取 tensor）。



## 基础教程

### MNIST 机器学习入门

MNIST 是一个入门级的计算机视觉数据集，它包含各种手写数字图片。

![](https://i.imgur.com/wAb2qch.png)

它也包含每一张图片对应的标签，告诉我们这个是数字几．比如，上面这四张图片的标签分别是 5,0,4,1．在此教程示例中，我们将训练一个机器学习模型用于预测图片里面的数字．我们的目的不是要设计一个世界一流的复杂模型---尽管我们会在之后给你源代码去实现一流的预测模型---而是要介绍下如何使用 TensorFlow．所以，我们这里会从一个很简单的数学模型开始，它叫做 Softmax Regression．

对应这个教程的实现代码很短，而且真正有意思的内容只包含在三行代码里面．但是，去理解包含在这些代码里面的设计思想是非常重要的：TensorFlow 工作流程和机器学习的基本概念．因此，这个教程会很详细地介绍这些代码的实现原理．

###  The MNIST Data | MNIST 数据集

MNIST 数据集的官网是 Yann LeCun’s website．在这里，我们提供了一份 python 源代码用于自动下载和安装这个数据集．可以下载参考连接中[MNIST input_data.py](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist)代码，然后用下面的代码导入到你的项目里面，也可以直接复制粘贴到你的代码文件里面．

下载下来的数据集可被分为三部分：55000 行训练用点数据集（mnist.train），10000 行测试数据集 (mnist.test)，以及 5000 行验证数据集（mnist.validation）．这样的切分很重要：在机器学习模型设计时必须有一个单独的测试数据集不用于训练而是用来评估这个模型的性能，从而更加容易把设计的模型推广到其他数据集上（泛化）．

正如前面提到的一样，每一个 MNIST 数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签．我们把这些图片设为“xs”，把这些标签设为“ys”．训练数据集和测试数据集都包含 xs 和 ys，比如训练数据集的图片是mnist.train.images ，训练数据集的标签是mnist.train.labels．每一张图片包含 28×28 像素．我们可以用一个数字数组来表示这张图片：

![](https://i.imgur.com/W4fwed4.png)

我们把这个数组展开成一个向量，长度是 28×28 = 784．如何展开这个数组（数字间的顺序）不重要，只要保持各个图片采用相同的方式展开．从这个角度来看，MNIST数据集的图片就是在 784 维向量空间里面的点, 并且拥有比较复杂的结构 (注意: 此类数据的可视化是计算密集型的)．

展平图片的数字数组会丢失图片的二维结构信息．这显然是不理想的，最优秀的计算机视觉方法会挖掘并利用这些结构信息，我们会在后续教程中介绍．但是在这个教程中我们忽略这些结构，所介绍的简单数学模型，softmax 回归 (softmax regression)，不会利用这些结构信息．

因此，在 MNIST 训练数据集中，mnist.train.images 是一个形状为 [55000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点．在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于 0 和 1 之间．

![](https://i.imgur.com/Ophbk2r.png)

相对应的 MNIST 数据集的标签是介于 0 到 9 的数字，用来描述给定图片里表示的数字．为了用于这个教程，我们使标签数据是"one-hot vectors"．一个 one-hot 向量除了某一位的数字是 1 以外其余各维度数字都是 0．所以在此教程中，数字 n 将表示成一个只有在第 n 维度（从 0 开始）数字为 1 的 10 维向量．比如，标签 0 将表示成 ([1,0,0,0,0,0,0,0,0,0,0])．因此，mnist.train.labels是一个 [55000, 10] 的数字矩阵．

![](https://i.imgur.com/vnrsd1f.png)

### Softmax 回归介绍

我们知道 MNIST 数据集的每一张图片都表示一个 (0 到 9 的) 数字．那么，如果模型若能看到一张图就能知道它属于各个数字的对应概率就好了。比如，我们的模型可能看到一张数字"9" 的图片，就判断出它是数字"9" 的概率为 80%，而有 5% 的概率属于数字"8"（因为 8 和 9 都有上半部分的小圆），同时给予其他数字对应的小概率（因为该图像代表它们的可能性微乎其微）．

这是能够体现 softmax 回归自然简约的一个典型案例．softmax 模型可以用来给不同的对象分配概率．在后文，我们训练更加复杂的模型时，最后一步也往往需要用 softmax 来分配概率．

softmax 回归（softmax regression）分两步：首先对输入被分类对象属于某个类的“证据”相加求和，然后将这个“证据”的和转化为概率.

我们使用加权的方法来累积计算一张图片是否属于某类的“证据”。如果图片的像素强有力的体现该图不属于某个类，则权重为负数，相反如果这个像素拥有有利的证据支持这张图片属于这个类，那么权值为正．

下面的图片显示了一个模型学习到的图片上每个像素对于特定数字类的权值．红色代表负权值，蓝色代表正权值．

![](https://i.imgur.com/5vdEquI.png)

我们也需要引入额外的“证据”，可称之为偏置量 (bias）。总的来说，我们希望它代表了与所输入向无关的判断证据．因此对于给定的输入图片 x 代表某数字 i 的总体证据可以表示为:

![](https://i.imgur.com/6HLI1WR.png)

其中，Wi 代表权重，bi 代表第 i 类的偏置量，j 代表给定图片 x 的像素索引用于像素求和．然后用 softmax 函数可以把这些证据转换成概率 y:

![](https://i.imgur.com/sFkvaMN.png)

这里的 softmax 可以看成是一个激励（activation）函数或是链接（link）函数，把我们定义的线性函数的输出转换成我们想要的格式，也就是关于 10 个数字类的概率分布．因此，给定一张图片，它对于每一个数字的吻合度可以被 softmax 函数转换成为一个概率值．softmax 函数可以定义为：

![](https://i.imgur.com/xcVpgQG.png)

展开等式右边的子式，可以得到：

![](https://i.imgur.com/8YaCrk6.png)

但是更多的时候把 softmax 模型函数定义为第一种形式：把输入值当成幂指数求值，再正则化这些结果值．这个幂运算表示，更大的证据对应更大的假设模型（hypothesis）里面的乘数权重值．反之，拥有更少的证据意味着在假设模型里面拥有更小的乘数系数．假设模型里的权值不可以是 0 值或者负值．Softmax 然后会正则化这些权重值，使它们的总和等于 1，以此构造一个有效的概率分布．（更多的关于 Softmax 函数的信息，可以参考 Michael Nieslen 的书里面的[这个](http://neuralnetworksanddeeplearning.com/chap3.html#softmax)部分，其中有关于 softmax 的可交互式的可视化解释．）

对于 softmax 回归模型可以用下面的图解释，对于输入的 xs 加权求和，再分别加上一个偏置量，最后再输入到 softmax 函数中：

![](https://i.imgur.com/08hsxEv.png)

如果把它写成一个方程，可以得到：

![](https://i.imgur.com/CHhWc2r.png)

我们也可以用向量表示这个计算过程：用矩阵乘法和向量相加．这有助于提高计算效率（也是一种更有效的思考方式）．

![](https://i.imgur.com/fAF6a95.png)

更进一步，可以写成更加紧凑的方式：

![](https://i.imgur.com/znFIU8X.png)

### 实现回归模型

为了在 python 中高效的进行数值计算，我们通常会调用（如 NumPy）外部函数库，把类似矩阵乘法这样的复杂运算使用其他外部语言实现．不幸的是，从外部计算切换回 Python 的每一个操作，仍然是一个很大的开销．如果你用 GPU 来进行外部计算，这样的开销会更大．用分布式的计算方式，也会花费更多的资源用来传输数据．

TensorFlow 也把复杂的计算放在 python 之外完成，但是为了避免前面说的那些开销，它做了进一步完善．TensorFlow 不单独地运行单一的复杂计算，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在 Python 之外运行．（这样类似的运行方式，可以在不少的机器学习库中看到．）

使用 TensorFlow 之前，首先导入它：

```python
import tensorflow as tf
```

我们通过操作符号变量来描述这些可交互的操作单元，可以用下面的方式创建一个：

```python
x = tf.placeholder("float", [None, 784])
```

x 不是一个特定的值，而是一个占位符 placeholder，我们在 TensorFlow 运行计算时输入这个值．我们希望能够输入任意数量的 MNIST 图像，每一张图展平成 784 维的向量．我们用 2 维的浮点数张量来表示这些图，这个张量的形状是 [None，784]．（这里的 None 表示此张量的第一个维度可以是任何长度的．）

我们的模型也需要权重值和偏置量，当然我们可以把它们当做是另外的输入（使用占位符），但 TensorFlow 有一个更好的方法来表示它们：Variable．一个Variable代表一个可修改的张量，存在在 TensorFlow 的用于描述交互性操作的图中．它们可以用于计算输入值，也可以在计算中被修改．对于各种机器学习应用，一般都会有模型参数，可以用Variable表示． 

```python
W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))
```

我们赋予tf.Variable 不同的初值来创建不同的Variable：在这里，我们都用全为零的张量来初始化W和b．因为我们要学习W和b的值，它们的初值可以随意设置。

注意，W的维度是[784，10]，因为我们想要用 784 维的图片向量乘以它以得到一个10 维的证据值向量，每一位对应不同数字类．b的形状是[10]，所以我们可以直接把它加到输出上面．

现在，可以实现我们的模型了，只需以下一行代码：

```python
y = tf.nn.softmax(tf.matmul(x, W) + b)
```

首先，我们用tf.matmul(X，W)表示 x 乘以 W ，对应之前等式里面的 Wx，这里 x 是一个 2 维张量拥有多个输入．然后再加上 b，把和输入到tf.nn.softmax函数里面．

至此，我们先用了几行简短的代码来设置变量，然后只用了一行代码来定义我们的模型．TensorFlow 不仅仅可以使 softmax 回归模型计算变得特别简单，它也用这种非常灵活的方式来描述其他各种数值计算，从机器学习模型对物理学模拟仿真模型．一旦被定义好之后，我们的模型就可以在不同的设备上运行：计算机的 CPU，GPU，甚至是手机！

### 训练模型

为了训练我们的模型，我们首先需要定义一个指标来评估这个模型是好的．其实，在机器学习，我们通常定义指标来表示一个模型是坏的，这个指标称为成本（cost）或损失（loss），然后尽量最小化这个指标．但是，这两种方式是相同的．

一个非常常见的，非常漂亮的成本函数是“交叉熵”(cross-entropy)．交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段．它的定义如下：

![](https://i.imgur.com/AeOF1f6.png)

y 是我们预测的概率分布,y′ 是实际的分布（我们输入的 one-hot vector)．比较粗糙的理解是，交叉熵是用来衡量我们的预测用于描述真相的低效性．更详细的关于交叉熵的解释超出本教程的范畴，但是你很有必要好好理解它．

为了计算交叉熵，我们首先需要添加一个新的占位符用于输入正确值

```python
y_ = tf.placeholder("float", [None, 10])
```

然后我们可以用以上公式计算交叉熵:

```python
cross_entropy = −tf.reduce_sum(y_*tf.log(y))
```

首先，用 tf.log 计算 y 的每个元素的对数．接下来，我们把 y_ 的每一个元素和 tf.log(y_) 的对应元素相乘．最后，用 tf.reduce_sum 计算张量的所有元素的总和．

值得注意的是，这里的交叉熵不仅仅用来衡量单一的一对预测和真实值，而是所有100 幅图片的交叉熵的总和．对于 100 个数据点的预测表现比单一数据点的表现能更好地描述我们的模型的性能．

现在我们知道我们需要我们的模型做什么啦，用 TensorFlow 来训练它是非常容易
的．因为 TensorFlow 拥有一张描述你各个计算单元的图，它可以自动地使用反向传播算法 (backpropagation algorithm)来有效地确定你的变量是如何影响你想要最小化的那个成本值的．然后，TensorFlow 会用你选择的优化算法来不断地修改变量以降低成本．

```python
train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
```

在这里，我们要求 TensorFlow 用梯度下降算法（gradient descent algorithm）以 0.01的学习速率最小化交叉熵．梯度下降算法（gradient descent algorithm）是一个简单的学习过程，TensorFlow 只需将每个变量一点点地往使成本不断降低的方向移动．当然 TensorFlow 也提供了其他许多优化算法：只要简单地调整一行代码就可以使用其他的算法．

TensorFlow 在这里实际上所做的是，它会在后台给描述你的计算的那张图里面增加一系列新的计算操作单元用于实现反向传播算法和梯度下降算法．然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本．

现在，我们已经设置好了我们的模型．在运行计算之前，我们需要添加一个操作来初始化我们创建的变量：

```python
init = tf.initialize_all_variables()
```

现在我们可以在一个 Session 里面启动我们的模型，并且初始化变量：

```python
sess = tf.Session()
sess.run(init)
```

然后开始训练模型，这里我们让模型循环训练 1000 次！

```python
for i in range(1000):
	batch_xs, batch_ys = mnist.train.next_batch(100)
	sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
```

该循环的每个步骤中，我们都会随机抓取训练数据中的 100 个批处理数据点，然后我们用这些数据点作为参数替换之前的占位符来运行 train_step．

使用一小部分的随机数据来进行训练被称为随机训练 (stochastic training)---在这里更确切的说是随机梯度下降训练．理想情况下，我们希望用我们所有的数据来进行每一步的训练，因为这能给我们更好的训练结果，但显然这需要很大的计算开销．所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性．

### Evaluating Our Model || 评估我们的模型

那么我们的模型性能如何呢？首先让我们找出那些预测正确的标签．tf.argmax()是一个非常有用的函数，它能给你在一个张量里沿着某条轴的最高条目的索引值．比如，tf.argmax(y,1)是模型认为每个输入最有可能对应的那些标签，而tf.argmax(y_,1)代表正确的标签．我们可以用tf.equal来检测我们的预测是否真实标签匹配．

```python
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
```

这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，`[True, False, True, True]` 会变成 `[1,0,1,1]` ，取平均值后得到 `0.75`.

```python
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
```

最后，我们计算所学习到的模型在测试数据集上面的正确率。

```python
print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})
```

这个最终结果值应该大约是91%。

这个结果好吗？嗯，并不太好。事实上，这个结果是很差的。这是因为我们仅仅使用了一个非常简单的模型。不过，做一些小小的改进，我们就可以得到97％的正确率。最好的模型甚至可以获得超过99.7％的准确率！（想了解更多信息，可以看看这个关于各种模型的[性能对比列表](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)。)



## 参考与感谢

1. 感谢**高博士**的指点与对我困惑的耐心解答
2. [谷歌全新开源人工智能系统TensorFlow官方文档中文版](https://github.com/jikexueyuanwiki/tensorflow-zh)
3. [Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2](https://blog.csdn.net/CliuGeek/article/details/78836598)
4. [StackOverflow Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2](https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u)
5. [tensorflow github](https://github.com/tensorflow/tensorflow)
6. [MNIST input_data.py](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist)
7. [Softmax回归](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)
8. [Softmax 函数的特点和作用是什么？](https://www.zhihu.com/question/23765351)
9. [tensorflow API](http://www.tensorfly.cn/tfdoc/api_docs/python/constant_op.html)
10. [Tensorflow-MNIST入门实例](https://blog.csdn.net/m0_37306360/article/details/74910173)