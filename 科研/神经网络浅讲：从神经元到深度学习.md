# [神经网络浅讲：从神经元到深度学习](http://www.cnblogs.com/subconscious/p/5058741.html)

神经网络参考文章，非原创。

 

## 一. 前言

　　让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是**输入层**，绿色的是**输出层**，紫色的是**中间层**（也叫**隐藏层**）。输入层有3个输入单元，隐藏层有4个单元，输出层有2个单元。后文中，我们统一使用这种颜色来表达神经网络的结构。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219151604318-1557737289.jpg)

 

　　在开始介绍前，有一些知识可以先记在心里：

1. 设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；
2. 神经网络结构图中的拓扑与箭头代表着**预测**过程时数据的流向，跟**训练**时的数据流有一定的区别；
3. 结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的**权重**（其值称为**权值**），这是需要训练得到的。  

　　除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219174631693-775181930.jpg) 

　　从左到右的表达形式以Andrew Ng和LeCun的文献使用较多，Caffe里使用的则是从下到上的表达。在本文中使用Andrew Ng代表的从左到右的表达形式。

　　下面从简单的神经元开始说起，一步一步介绍神经网络复杂结构的形成。

 

## 二. 神经元

### 1.结构

　　神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。

　　下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。

　　注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219153856802-307732621.jpg)

 

　　连接是神经元中最重要的东西。每一个连接上都有一个权重。

　　**一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。**

　　我们使用a来表示输入，用 w 来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是 a，端中间有加权参数 w，经过这个加权后的信号会变成 a * w，因此在连接的末端，信号的大小就变成了 a * w。

　　在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219180614819-1652574235.jpg)

 

　　如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230201441792-1505283920.jpg)

 

　　可见 z 是在输入和权值的线性加权和叠加了一个**函数 g** 的值。在 MP 模型里，函数 g 是 sgn 函数，也就是取符号函数。这个函数当输入大于 0 时，输出 1，否则输出 0。

　　下面对神经元模型的图进行一些扩展。首先将 sum 函数与 sgn 函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入 a 与输出 z 写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。

　　神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204036479-461440948.jpg)

 

　　当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“**单元**”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“**节点**”（node）来表达同样的意思。 

### 2.效果 

　　神经元模型的使用可以这样理解：

　　我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性**预测**未知属性。

　　具体办法就是使用神经元的公式进行计算。三个已知属性的值是 a1，a2，a3，未知属性的值是 z。z 可以通过公式计算出来。

　　这里，已知的属性称之为**特征**，未知的属性称之为**目标**。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值 w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。

 

## 三. 单层神经网络（感知器）

### 1.结构

　　下面来说明感知器模型。

　　在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值w1, w2, w3写到“连接线”的中间。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151221151959015-1876891081.jpg)

 

　　在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。

　　我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们**根据计算层的数量来命名**。

　　假如我们要预测的目标不再是一个值，而是一个向量，例如 [2,3]。那么可以在输出层再增加一个“输出单元”。

　　下图显示了带有两个输出单元的单层神经网络，其中输出单元 z1 的计算公式如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204223917-579926148.jpg)

　　可以看到，z1的计算跟原先的z并没有区别。

　　我们已知一个神经元的输出可以向多个神经元传递，因此z2的计算公式如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204258057-82126781.jpg)

　　可以看到，z2的计算中除了三个新的权值：w4，w5，w6 以外，其他与z1是一样的。

　　整个网络的输出如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204606760-610264555.jpg)

　　目前的表达公式有一点不让人满意的就是：w4，w5，w6是后来加的，很难表现出跟原先的w1，w2，w3的关系。

　　因此我们改用二维的下标，用 Wx,y来表达一个权值。下标中的x代表后一层神经元的序号，而y代表前一层神经元的序号（序号的顺序从上到下）。

　　例如，W1,2 代表后一层的第 1 个神经元与前一层的第 2 个神经元的连接的权值（这种标记方式参照了Andrew Ng的课件）。根据以上方法标记，我们有了下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230205437995-673856644.jpg)

　　如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。

　　例如，输入的变量是 [a1，a2，a3]T（代表由a1，a2，a3组成的列向量），用向量 **a** 来表示。方程的左边是 [z1，z2]T，用向量 **z** 来表示。

　　系数则是矩阵 **W**（2行3列的矩阵，排列形式与公式中的一样）。

　　于是，输出公式可以改写成：
$$
g(W * *) = z
$$
 

　　这个公式就是神经网络中从前一层计算后一层的**矩阵运算。**

### 2.效果

　　与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个 **逻辑回归** 模型，可以做线性分类任务。

　　我们可以用 **决策分界** 来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。

　　下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231073138323-962584420.png)

### 3.影响

　　感知器只能做简单的线性分类任务。但是当时的人们热情太过于高涨，并没有人清醒的认识到这点。于是，当人工智能领域的巨擘Minsky指出这点时，事态就发生了变化。

　　Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。

## 四. 两层神经网络（多层感知器）

### 1.引子

　　当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。

　　1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。 

### 2.结构

　　两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。

　　现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。

　　例如ax(y)代表第y层的第x个节点。z1，z2变成了a1(2)，a2(2)。下图给出了a1(2)，a2(2)的计算公式。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222164731249-360921014.jpg)

　　计算最终输出z的方式是利用了中间层的a1(2)，a2(2)和第二个权值矩阵计算得到的，如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222171056156-387680541.jpg)

　　假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。

　　我们使用向量和矩阵来表示层次中的变量。**a**(1)，**a**(2)，**z** 是网络中传输的向量数据。**W**(1)和**W**(2)是网络的矩阵参数。如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222171328140-1303075636.jpg)

 

使用矩阵运算来表达整个计算公式的话如下：
$$
g(W(1) * a(1)) = a(2)
$$

$$
g(W(2) * a(2)) = z
$$

　　由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。

　　需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（bias unit）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。

　　偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量 **b**，称之为偏置。如下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151226111144687-604911384.jpg)

　　可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。 

　　在考虑了偏置以后的一个神经网络的矩阵运算如下：
$$
g(W(1) * a(1) + b(1)) = a(2);
$$

$$
g(W(2) * a(2) + b(2)) = z;
$$

　　需要说明的是，在两层神经网络中，我们不再使用 sgn 函数作为函数 g，而是使用平滑函数sigmoid 作为函数 g。我们把函数 g 也称作激活函数（active function）。

　　事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。

### 3.效果

　　与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。

　　这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。

　　下面就是一个例子（此两图来自colah的[博客](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)），红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是决策分界。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231073619073-461403542.png)

　　可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。有趣的是，前面已经学到过，单层网络只能做线性分类任务。而两层神经网络中的后一层也是线性分类层，应该只能做线性分类任务。为什么两个线性分类任务结合就可以做非线性分类任务？

　　我们可以把输出层的决策分界单独拿出来看一下。就是下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231074314604-2050732128.png)

　　可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。

　　这样就导出了两层神经网络可以做非线性分类的关键--隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。

　　两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。

　　下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）。

　　了解了两层神经网络的结构以后，我们就可以看懂其它类似的结构图。例如EasyPR字符识别网络架构（下图）。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151226122337406-1923048422.jpg)

　　EasyPR使用了字符的图像去进行字符文字的识别。输入是120维的向量。输出是要预测的文字类别，共有65类。根据实验，我们测试了一些隐藏层数目，发现当值为40时，整个网络在测试集上的效果较好，因此选择网络的最终结构就是120，40，65。

### 4.训练

　　下面简单介绍一下两层神经网络的训练。

　　机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为yp，真实目标为y。那么，定义一个值loss，计算公式如下。
$$
loss = (yp - y)^2
$$
　　这个值称之为**损失**（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。

　　如果将先前的神经网络预测的矩阵公式带入到 yp 中（因为有 z = yp），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为**损失函数**（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。

　　此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于 0 的运算量很大，所以一般来说解决这个优化问题使用的是**梯度下降**算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。

　　在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用**反向传播**算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。

　　反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151229120754198-2003498733.jpg)



　　反向传播算法的启示是数学中的**链式法则**。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从 BP 算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。

　　优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做**泛化**（generalization），相关方法被称作正则化（regularization）。神经网络中常用的泛化技术有**权重衰减**等。

### 5.影响

　　两层神经网络在多个地方的应用说明了其效用与价值。10年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。

## 五. 多层神经网络（深度学习）

### 1.引子　　

　　2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“**预训练**”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“**微调**”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词--“**深度学习**”。

### 2.结构

　　我们延续两层神经网络的方式来设计一个多层神经网络。

　　在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224204339234-1994620313.jpg)

图30 多层神经网络

 

　　依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。

　　在已知输入**a**(1)，参数**W**(1)，**W**(2)，**W**(3)的情况下，输出**z**的推导公式如下：
$$
g(W(1) * a(1)) = a(2);
$$

$$
g(W(2) * a(2)) = a(3);
$$

$$
g(W(3) * a(3)) = z;
$$

　　多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。

　　下面讨论一下多层神经网络中的参数。

　　首先我们看第一张图，可以看出**W**(1)中有6个参数，**W**(2)中有4个参数，**W**(3)中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224212531484-570745053.jpg) 

　　假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。

　　经过调整以后，整个网络的参数变成了33个。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224213620234-1075501325.jpg) 

　　虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质，下面会做介绍。

　　在参数一致的情况下，我们也可以获得一个“更深”的网络。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224213703109-813423001.jpg) 

　　上图的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。

### 3.效果

　　与两层层神经网络不同。多层神经网络中的层数增加了很多。

　　增加更多的层次有什么好处？**更深入的表示特征，以及更强的函数模拟能力**。

　　更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。

　　关于逐层特征学习的例子，可以参考下图。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231075103229-1126297331.png) 

　　更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的**容量**（capcity）去拟合真正的关系。

　　通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。

### 4.训练

　　在单层神经网络时，我们使用的激活函数是 sgn 函数。到了两层神经网络时，我们使用的最多的是 sigmoid 函数。而到了多层神经网络时，通过一系列的研究发现，ReLU 函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是 ReLU 函数。ReLU 函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是 y=max(x,0)。简而言之，在 x 大于 0，输出就是输入，而在 x 小于 0 时，输出就保持为 0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。

　　在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现**过拟合现象**。因此正则化技术就显得十分重要。目前，Dropout 技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。



　　



　　其实本文的名字“神经网络浅讲”并不合适，因为本文并不是讲的是“神经网络”的内容，而是其中的一个子类，也是目前最常说的**前馈神经网络**。根据下图的分类可以看出。

![img](https://images2015.cnblogs.com/blog/673793/201512/673793-20151227190543499-2614280.jpg) 



**参考文献：**

　　1.[Neural Networks](https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html)

　　2.[Andrew Ng Neural Networks ](http://ufldl.stanford.edu/wiki/index.php/Neural_Networks)

　　3.[神经网络简史](http://www.36dsj.com/archives/20804)

　　4.[中科院 史忠植 神经网络 讲义](http://www.intsci.ac.cn/shizz/course/kd08.ppt)

　　5.[深度学习 胡晓林](http://caai.cn/contents/118/1934.html)